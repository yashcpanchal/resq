<p align="center">
  <img height="100" alt="Actian" src="https://www.actian.com/wp-content/themes/hcl-actian/images/actian-logo.svg">
  &nbsp;
</p>

<p align="center">
    <b>Actian VectorAI DB</b>
</p>

# Actian VectorAI DB and Python client

The Actian VectorAI DB and Python client. Please review the [Known Issues](#-known-issues) section before deploying.

## What is VectorAI DB?

Actian VectorAI DB is a vector database ‚Äî a specialized database for AI applications that search by **meaning**, not just keywords. Think of it like this:
- Regular database: "Find products named 'laptop'"
- Vector database: "Find products *similar* to 'portable computer for students'"

**Common use cases:** RAG chatbots, semantic search, recommendation engines, anomaly detection.

**Important:** VectorAI DB does **not** include an embedding model. You need to bring your own (e.g. `sentence-transformers`, OpenAI embeddings). It handles storage and search ‚Äî you handle the embeddings.

**About gRPC:** The database communicates over gRPC under the hood ‚Äî you don't need to know anything about it. The Python client handles it for you. If you see gRPC in an error message, it's a connection issue, not a code issue.

### Supported platforms

* The VectorAI DB Docker image is currently supported only on Linux/amd64 (x86_64).
    * Supported on Windows using WSL2 (Docker Desktop or Podman Desktop).
    * macOS support on Apple chipsets (M2/M3/M4) through Rosetta 2 and specifying linux/amd64 as platform. 
        * Install Rosetta 2 by running `softwareupdate --install-rosetta --agree-to-license`.
        * Add `--platform linux/amd64` to Docker commands.

* The Python client package is supported on all major platforms (Windows, macOS, and Linux).
    * Python 3.10 or higher is required.

## Features

- üöÄ **Async & Sync clients** - Full async/await support with `AsyncCortexClient`
- üîê **Persistent storage** - Production-grade data persistence
- üîç **Type-safe Filter DSL** - Fluent API for payload filtering
- ‚ö° **Smart Batching** - Automatic request batching for high throughput
- üì¶ **Pydantic models** - Type hints and validation throughout
- üéØ **gRPC transport** - High-performance communication

## Quick Install ‚Äì Pull from DockerHub

1. Make sure you have [Docker](https://docs.docker.com/get-docker/) installed. **Note to Mac users with Apple Silicon:** Docker Desktop automatically handles running this amd64 image on your ARM Mac.

2. Clone this repository.

3. Start the database:

```bash
   docker compose up
```

   Or run in the background:

```bash
   docker compose up -d
```

  The database will be available at `localhost:50051`. The docker-compose.yml file handles the base config required.

4. To stop the container:

```bash
   docker compose down
```

## üì• Docker container installation ‚Äì with the .tar image file (not included in this repository)

Load the container archive into your container environment:

```bash
docker image load -i Actian_VectorAI_DB_Beta.tar
```

### Container ports and volumes

The container exposes port `50051` and stores its logs and persisted collections in the `/data` directory, which you should map to a host directory to persist data outside the container.

**Port conflict?** If port `50051` is already in use, change the host port in the compose file:
```yaml
ports:
  - "50052:50051"  # Use any free port on the left side
```
Then connect with `CortexClient("localhost:50052")`.

### Deploy container with Docker run

To deploy the container using `docker run`:

```bash
docker run -d --name vectoraidb -v ./data:/data -p 50051:50051 localhost/actian/vectoraidb:1.0b
```

### Deploy container with Docker compose

To deploy the container using `docker compose`, create a `docker-compose.yml` file with this service definition and start it with `docker compose up`.

```yaml
services:
    vectoraidb:
       image: localhost/actian/vectoraidb:1.0b
       #platform: linux/amd64   # Uncomment on macOS
       container_name: vectoraidb
       ports:
         - "50051:50051"
       volumes:
         - ./data:/data
       restart: unless-stopped
       stop_grace_period: 2m
```

_Note: Collections and logs are persisted under the mounted /data directory_

### Examine container logs

The VectorAI DB server writes useful informational messages and errors to its log. These logs are often the best place to start when diagnosing failed requests or unexpected behavior.

You can access the server logs in two ways:

- Use `docker logs <container-name>` to stream or inspect the container logs directly.
- Read the log file at `/data/vde.log` from the host directory you mapped to `/data` when starting the container.

## üì• Python environment setup

Here's how to create and activate a virtual environment:

**Linux/macOS:**
```bash
python -m venv .venv
source .venv/bin/activate
```

**Windows (Command Prompt):**
```cmd
python -m venv .venv
.venv\Scripts\activate.bat
```

**Windows (PowerShell):**
```powershell
python -m venv .venv
.venv\Scripts\Activate.ps1
```

## üì• Install Python client

Install the Python client with pip:

```bash
pip install actiancortex-0.1.0b1-py3-none-any.whl
```

**_For detailed API documentation, see [docs/api.md](./docs/api.md)._**

## üöÄ Quickstart

Sync client and async client quickstarts are available.

### Sync client

```python
from cortex import CortexClient, DistanceMetric

with CortexClient("localhost:50051") as client:
    # Health check
    version, uptime = client.health_check()
    print(f"Connected to {version}")

    # Create collection
    client.create_collection(
        name="products",
        dimension=128,
        distance_metric=DistanceMetric.COSINE,
    )

    # Insert vectors
    client.upsert("products", id=0, vector=[0.1]*128, payload={"name": "Product A"})

    # Batch insert
    client.batch_upsert(
        "products",
        ids=[1, 2, 3],
        vectors=[[0.2]*128, [0.3]*128, [0.4]*128],
        payloads=[{"name": f"Product {i}"} for i in [1, 2, 3]],
    )

    # Search
    results = client.search("products", query=[0.1]*128, top_k=5)
    for r in results:
        print(f"ID: {r.id}, Score: {r.score}")

    # Cleanup
    client.delete_collection("products")
```

### Async client

```python
import asyncio
from cortex import AsyncCortexClient

async def main():
    async with AsyncCortexClient("localhost:50051") as client:
        # All methods are async
        await client.create_collection("demo", 128)
        await client.upsert("demo", id=0, vector=[0.1]*128)
        results = await client.search("demo", [0.1]*128, top_k=5)
        await client.delete_collection("demo")

asyncio.run(main())
```

**Note:** To use the sync `CortexClient` in async contexts (e.g., MCP servers, FastAPI), wrap calls with `asyncio.to_thread()`. For fully async code, use `AsyncCortexClient` instead.

## What Can I Do With Retrieved Vectors?

Once you retrieve results, the `payload` is where your real data lives. Some ideas:
- **RAG:** Feed the retrieved text chunks into an LLM (OpenAI, Ollama, etc.) as context
- **Recommendations:** Use the returned IDs to look up items in your own database
- **Anomaly detection:** Flag results whose `score` falls below a similarity threshold
- **Note-taking / personal search:** Store notes as vectors, search them by meaning later

Results look like this:
```python
for result in results:
    print(result.score)    # similarity score
    print(result.payload)  # your original data, e.g. {"text": "...", "source": "..."}
```

## üìö Core API

### Collection management

| Method                                      | Description              |
| ------------------------------------------- | ------------------------ |
| `create_collection(name, dimension, ...)`   | Create new collection    |
| `delete_collection(name)`                   | Delete collection        |
| `has_collection(name)`                      | Check if exists          |
| `collection_exists(name)`                   | Alias for has_collection |
| `recreate_collection(name, dimension, ...)` | Delete and recreate      |
| `open_collection(name)`                     | Open for operations      |
| `close_collection(name)`                    | Close collection         |

### Vector operations

| Method                                             | Description                 |
| -------------------------------------------------- | --------------------------- |
| `upsert(collection, id, vector, payload)`          | Insert/update single vector |
| `batch_upsert(collection, ids, vectors, payloads)` | Batch insert                |
| `get(collection, id)`                              | Get vector by ID            |
| `get_many(collection, ids)`                        | Get multiple vectors        |
| `retrieve(collection, ids)`                        | Alias for get_many          |
| `delete(collection, id)`                           | Delete vector               |
| `count(collection)`                                | Get vector count            |
| `scroll(collection, limit, cursor)`                | Paginate through vectors    |

### Search operations

| Method                                              | Description     |
| --------------------------------------------------- | --------------- |
| `search(collection, query, top_k)`                  | K-NN search     |
| `search_filtered(collection, query, filter, top_k)` | Filtered search |

### Maintenance

| Method                  | Description         |
| ----------------------- | ------------------- |
| `flush(collection)`     | Flush to disk       |
| `get_stats(collection)` | Get statistics      |
| `health_check()`        | Check server health |

## üîç Filter DSL

Type-safe filter building for payload queries:

```python
from cortex.filters import Filter, Field

# Simple conditions
filter = Filter().must(Field("category").eq("electronics"))

# Range conditions
filter = Filter().must(Field("price").range(gte=100, lte=500))

# Combined conditions
filter = (
    Filter()
    .must(Field("category").eq("electronics"))
    .must(Field("price").lte(500))
    .must_not(Field("deleted").eq(True))
)

# Use in search
results = client.search_filtered("products", query_vector, filter, top_k=10)
```

## üìñ Examples

### üåü Featured: End-to-End RAG Example

**New to vector databases?** Start here! The RAG example shows how VectorAI DB integrates into a complete AI application:

```bash
# Install dependencies
pip install -r examples/rag/requirements.txt

# Run the RAG example
python examples/rag/rag_example.py
```

See [examples/rag/README.md](./examples/rag/README.md) for a detailed walkthrough of building a Retrieval-Augmented Generation application.

### Other Examples

```bash
# Quick start - recommended starting point
python examples/quick_start.py

# Semantic search with filtering
python examples/semantic_search.py

# Async operations
python examples/async_example.py

# Batch operations
python examples/batch_upsert.py
```

## üìä Storage

Cortex uses persistent storage as the default backend. This provides:

- ‚úÖ Production-grade persistence
- ‚úÖ Transactional safety
- ‚úÖ High-performance I/O

## üîß Configuration

### HNSW parameters

```python
client.create_collection(
    name="vectors",
    dimension=128,
    hnsw_m=32,              # Edges per node (default: 16)
    hnsw_ef_construct=256,  # Build-time neighbors (default: 200)
    hnsw_ef_search=100,     # Search-time neighbors (default: 50)
)
```

### Distance metrics

- `COSINE` - Cosine similarity (default, recommended for normalized vectors)
- `EUCLIDEAN` - L2 distance
- `DOT` - Dot product

## üì¶ Dependencies

- `grpcio>=1.68.1` - gRPC transport
- `protobuf>=5.29.2` - Protocol buffers
- `numpy>=2.2.1` - Vector operations
- `pydantic>=2.10.4` - Data validation

## üêû Known issues

- CRTX-202: Closing or deleting collections while read/write operations are in progress is not supported.
- CRTX-232: scroll API uses the term cursor to indicate the offset.
- CRTX-233: get_many API does not return the vector IDs.

## üìÑ License

Proprietary - Actian Corporation

---

<p align="center">
  <b>Copyright ¬© 2025-2026 Actian Corporation. All Rights Reserved.</b>
</p>


