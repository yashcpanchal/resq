# ğŸš€ End-to-End RAG Example

This example demonstrates how **Actian VectorAI DB** integrates into a complete **Retrieval-Augmented Generation (RAG)** application. Perfect for developers new to vector databases!

## ğŸ¯ What You'll Learn

This example shows the **complete workflow** of a RAG application:

1. **ğŸ“„ Document Processing** - Loading and chunking text into manageable pieces
2. **ğŸ§  Embedding Generation** - Converting text to vectors using a real ML model
3. **ğŸ’¾ Vector Storage** - Storing embeddings in VectorAI DB with metadata
4. **ğŸ” Query Processing** - Converting user questions into embeddings
5. **ğŸ” Semantic Search** - Finding relevant context using vector similarity
6. **âœ¨ Answer Generation** - Using retrieved context to generate accurate answers

## ğŸ—ï¸ How It Works

```
User Query
    â†“
[1] Convert query to embedding
    â†“
[2] Search VectorAI DB for similar vectors
    â†“
[3] Retrieve relevant text chunks
    â†“
[4] Generate answer using context
    â†“
Answer to User
```

## ğŸ“¦ Installation

### Prerequisites

1. **VectorAI DB Running:**
   ```bash
   docker compose up -d
   ```

2. **Install Python Dependencies:**
   ```bash
   pip install -r examples/rag/requirements.txt
   ```

   Or install manually:
   ```bash
   pip install sentence-transformers openai
   ```

## ğŸ§ª Quick Test

Before running the full example, verify your setup:

```bash
python examples/rag/test_rag.py
```

This checks:
- âœ… Python version
- âœ… VectorAI DB is running
- âœ… Dependencies installed
- âœ… Database connection works

## ğŸš€ Usage

### Local Mode (No API Key Required)

Run without an LLM - just demonstrates the retrieval part:

```bash
python examples/rag/rag_example.py --local
```

### With OpenAI (Recommended)

For full RAG with answer generation:

```bash
export OPENAI_API_KEY="your-api-key-here"  # Linux/macOS
# OR
set OPENAI_API_KEY=your-api-key-here       # Windows CMD
# OR  
$env:OPENAI_API_KEY="your-api-key-here"    # Windows PowerShell

python examples/rag/rag_example.py
```

### Custom Server

```bash
python examples/rag/rag_example.py --server your-host:50051
```

## ğŸ“– Example Output

```
ğŸš€ End-to-End RAG Example with Actian VectorAI DB
======================================================================

ğŸ“¥ Step 1: Loading embedding model...
   âœ“ Model loaded: all-MiniLM-L6-v2 (384 dimensions)

ğŸ”Œ Step 2: Connecting to VectorAI DB at localhost:50051...
   âœ“ Connected to VectorAI DB v1.0

ğŸ’¾ Step 3: Creating collection 'rag_demo_a3f5c2e1'...
   âœ“ Collection created with COSINE similarity

ğŸ“„ Step 4: Processing and chunking documents...
   âœ“ Created 12 text chunks

ğŸ§  Step 5: Generating embeddings and storing in database...
   âœ“ Stored 12 document chunks with embeddings
   âœ“ Verified: 12 vectors in database

ğŸ” RAG Query Examples
======================================================================

Query 1: What are the key features of Actian VectorAI DB?
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

  1ï¸âƒ£  Embedding query...
  2ï¸âƒ£  Retrieving relevant context from VectorAI DB...
     Found 3 relevant chunks:
       â€¢ Chunk 1 (similarity: 0.8542)
       â€¢ Chunk 2 (similarity: 0.7891)
       â€¢ Chunk 3 (similarity: 0.7234)
  3ï¸âƒ£  Generating answer...

  ğŸ“ Answer:
     Actian VectorAI DB offers several key features including async and 
     sync clients with full async/await support, persistent storage for 
     production-grade data persistence, and a type-safe Filter DSL...
```

## ğŸ§  Understanding the Code

### 1. Document Chunking

```python
chunks = chunk_text(KNOWLEDGE_BASE)
# Splits long documents into overlapping chunks
# Preserves context at chunk boundaries
```

### 2. Embedding Generation

```python
model = SentenceTransformer('all-MiniLM-L6-v2')
embeddings = model.encode(texts)
# Converts text to 384-dimensional vectors
```

### 3. Storing in VectorAI DB

```python
client.batch_upsert(COLLECTION, ids, vectors, payloads)
# Efficiently stores all vectors with metadata
```

### 4. Semantic Search

```python
query_embedding = model.encode([query])[0]
results = client.search(COLLECTION, query_embedding, top_k=3)
# Finds most similar vectors using cosine similarity
```

### 5. Context Retrieval & Answer Generation

```python
context = get_retrieved_chunks(results)
answer = generate_answer(query, context)
# Uses LLM to generate answer from retrieved context
```

## ğŸ’¡ Real-World Applications

This pattern is used in:

- ğŸ“š **Documentation Search** - Find relevant docs to answer user questions
- ğŸ¤– **Customer Support Bots** - Answer questions using company knowledge base
- ğŸ“° **Content Recommendations** - Find similar articles or products
- ğŸ”¬ **Research Tools** - Search scientific papers by semantic meaning
- ğŸ’¼ **Enterprise Search** - Search internal documents by intent, not keywords

## ğŸ“ Why Vector Databases?

Traditional databases use **exact matching**:
```
Query: "How to start the database?"
Match: WHERE text = "How to start the database?"  âŒ No results
```

Vector databases use **semantic similarity**:
```
Query: "How to start the database?"
Finds: "To get started with Actian VectorAI DB..."  âœ… Relevant match!
```

## ğŸ”§ Customization Tips

### Use Your Own Documents

Replace the `KNOWLEDGE_BASE` string with your own content:

```python
# Load from file
with open("my_docs.txt") as f:
    KNOWLEDGE_BASE = f.read()

# Load from multiple files
documents = []
for file in ["doc1.txt", "doc2.txt", "doc3.txt"]:
    with open(file) as f:
        documents.append(f.read())
KNOWLEDGE_BASE = "\n\n".join(documents)
```

### Try Different Embedding Models

```python
# Larger, more accurate model (768 dimensions)
model = SentenceTransformer('all-mpnet-base-v2')

# Multilingual model
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
```

### Adjust Chunk Size

```python
# Smaller chunks (more precise)
chunks = chunk_text(text, chunk_size=100, overlap=20)

# Larger chunks (more context)
chunks = chunk_text(text, chunk_size=500, overlap=100)
```

### Tune Search Parameters

```python
# Retrieve more context
results = client.search(COLLECTION, query_embedding, top_k=5)

# Better accuracy (slower)
client.create_collection(
    name=COLLECTION,
    dimension=DIMENSION,
    hnsw_ef_search=200  # Higher = more accurate
)
```

## ğŸ¤” Common Questions

**Q: Do I need OpenAI?**  
A: No! Run with `--local` flag to see how the retrieval works. You can integrate any LLM (Anthropic, local models, etc.)

**Q: Why use embeddings?**  
A: Embeddings capture semantic meaning, allowing you to find relevant information even with different wording.

**Q: How big can my knowledge base be?**  
A: VectorAI DB can handle millions of vectors. The example uses a small KB for demonstration.

**Q: Can I use this in production?**  
A: Yes! VectorAI DB provides persistent storage and production-grade performance. Scale up your document corpus and you're good to go.

**Q: What about real-time updates?**  
A: Use `client.upsert()` to add/update documents in real-time as your knowledge base grows.

## ğŸ“š Next Steps

1. **Try it with your own documents** - Replace the sample knowledge base
2. **Integrate with your LLM** - Swap OpenAI for Claude, Llama, etc.
3. **Add a web interface** - Build a simple Flask/FastAPI frontend
4. **Implement filters** - Use the Filter DSL to narrow search by metadata
5. **Scale up** - Load thousands of documents and test performance

## ğŸ†˜ Troubleshooting

**"Connection refused"**
```bash
# Make sure VectorAI DB is running
docker compose up -d
docker ps  # Should show vectoraidb container
```

**"Module not found: sentence_transformers"**
```bash
pip install sentence-transformers
```

**"OpenAI API error"**
```bash
# Check your API key is set
echo $OPENAI_API_KEY  # Linux/macOS
echo %OPENAI_API_KEY%  # Windows CMD

# Or run in local mode
python examples/rag/rag_example.py --local
```

## ğŸ‰ Success!

You now understand how VectorAI DB integrates into AI applications! The same pattern applies to any RAG use case - just swap in your data and LLM of choice.

---

**Questions?** Open an issue or check the [main documentation](../README.md)
